{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12094942,"sourceType":"datasetVersion","datasetId":7614001},{"sourceId":12098249,"sourceType":"datasetVersion","datasetId":7616372}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:16:29.511109Z","iopub.execute_input":"2025-06-15T08:16:29.511429Z","iopub.status.idle":"2025-06-15T08:16:29.794603Z","shell.execute_reply.started":"2025-06-15T08:16:29.511407Z","shell.execute_reply":"2025-06-15T08:16:29.794040Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/emnlp-test-enbodo-2025/en-bodo.txt\n/kaggle/input/engbodo/English-Bodo Traning Data 2025.xlsx\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q transformers datasets accelerate bitsandbytes peft evaluate sacrebleu\n!pip install -U bitsandbytes\n!pip install datasets pandas sacremoses fasttext sentence-transformers sentencepiece scikit-learn\n!wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz\n!pip install spacy indic-transliteration\n!python -m spacy download en_core_web_sm \n!pip install indic-nlp-library","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:16:29.795661Z","iopub.execute_input":"2025-06-15T08:16:29.795956Z","iopub.status.idle":"2025-06-15T08:18:26.488800Z","shell.execute_reply.started":"2025-06-15T08:16:29.795938Z","shell.execute_reply":"2025-06-15T08:18:26.488050Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.0)\nRequirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nCollecting sacremoses\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: fasttext in /usr/local/lib/python3.11/dist-packages (0.9.3)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2024.11.6)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.5.0)\nRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.13.6)\nRequirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.2.0)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sacremoses\nSuccessfully installed sacremoses-0.1.1\n--2025-06-15 08:18:05--  https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.51, 3.163.189.96, 3.163.189.14, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.51|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 938013 (916K) [binary/octet-stream]\nSaving to: ‘lid.176.ftz’\n\nlid.176.ftz         100%[===================>] 916.03K  --.-KB/s    in 0.03s   \n\n2025-06-15 08:18:05 (25.6 MB/s) - ‘lid.176.ftz’ saved [938013/938013]\n\nRequirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\nCollecting indic-transliteration\n  Downloading indic_transliteration-2.3.69-py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\nRequirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\nCollecting backports.functools-lru-cache (from indic-transliteration)\n  Downloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (2024.11.6)\nRequirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (0.10.2)\nCollecting roman (from indic-transliteration)\n  Downloading roman-5.0-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\nRequirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.0->spacy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.0->spacy) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.0->spacy) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.0->spacy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.0->spacy) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\nDownloading indic_transliteration-2.3.69-py3-none-any.whl (155 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl (6.7 kB)\nDownloading roman-5.0-py3-none-any.whl (5.5 kB)\nInstalling collected packages: roman, backports.functools-lru-cache, indic-transliteration\nSuccessfully installed backports.functools-lru-cache-2.0.0 indic-transliteration-2.3.69 roman-5.0\nCollecting en-core-web-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\nCollecting indic-nlp-library\n  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\nCollecting sphinx-argparse (from indic-nlp-library)\n  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: sphinx-rtd-theme in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (0.2.4)\nCollecting morfessor (from indic-nlp-library)\n  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (2.2.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->indic-nlp-library) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->indic-nlp-library) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->indic-nlp-library) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->indic-nlp-library) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->indic-nlp-library) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->indic-nlp-library) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2025.2)\nRequirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (8.2.3)\nRequirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\nRequirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\nRequirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\nRequirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\nRequirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\nRequirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\nRequirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\nRequirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.6)\nRequirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.19.1)\nRequirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\nRequirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.17.0)\nRequirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\nRequirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\nRequirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\nRequirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.0)\nRequirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (25.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->indic-nlp-library) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->indic-nlp-library) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->indic-nlp-library) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->indic-nlp-library) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->indic-nlp-library) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2025.4.26)\nDownloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m112.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\nDownloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\nInstalling collected packages: morfessor, sphinx-argparse, indic-nlp-library\nSuccessfully installed indic-nlp-library-0.92 morfessor-2.0.6 sphinx-argparse-0.5.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install indic-transliteration","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:18:26.489860Z","iopub.execute_input":"2025-06-15T08:18:26.490163Z","iopub.status.idle":"2025-06-15T08:18:29.707723Z","shell.execute_reply.started":"2025-06-15T08:18:26.490126Z","shell.execute_reply":"2025-06-15T08:18:29.706937Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: indic-transliteration in /usr/local/lib/python3.11/dist-packages (2.3.69)\nRequirement already satisfied: backports.functools-lru-cache in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (2.0.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (2024.11.6)\nRequirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (0.15.2)\nRequirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (0.10.2)\nRequirement already satisfied: roman in /usr/local/lib/python3.11/dist-packages (from indic-transliteration) (5.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer->indic-transliteration) (8.1.8)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer->indic-transliteration) (4.13.2)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer->indic-transliteration) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer->indic-transliteration) (14.0.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer->indic-transliteration) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer->indic-transliteration) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer->indic-transliteration) (0.1.2)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import (AutoTokenizer, AutoModelForSeq2SeqLM, \n                          Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq)\nfrom transformers import BitsAndBytesConfig\nfrom peft import LoraConfig, TaskType, get_peft_model\nimport evaluate ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:18:29.709586Z","iopub.execute_input":"2025-06-15T08:18:29.709886Z","iopub.status.idle":"2025-06-15T08:18:56.533060Z","shell.execute_reply.started":"2025-06-15T08:18:29.709846Z","shell.execute_reply":"2025-06-15T08:18:56.532502Z"}},"outputs":[{"name":"stderr","text":"2025-06-15 08:18:42.579243: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749975522.816667      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749975522.883096      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\n\nfile_path = \"/kaggle/input/engbodo/English-Bodo Traning Data 2025.xlsx\"\ndf = pd.read_excel(file_path)\n\ncsv_path = \"/kaggle/working/english_bodo_training_data_2025.csv\"\ndf.to_csv(csv_path, index=False)\n\nprint(f\"CSV file saved to: {csv_path}\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:19:47.496127Z","iopub.execute_input":"2025-06-15T08:19:47.496769Z","iopub.status.idle":"2025-06-15T08:19:48.996457Z","shell.execute_reply.started":"2025-06-15T08:19:47.496745Z","shell.execute_reply":"2025-06-15T08:19:48.995531Z"}},"outputs":[{"name":"stdout","text":"CSV file saved to: /kaggle/working/english_bodo_training_data_2025.csv\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import pandas as pd\ndf = df.drop(columns=[\"Unnamed: 2\", \"Unnamed: 3\"], errors='ignore')  # returns a new DataFrame\ndf.to_csv(\"bodo.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:19:50.546052Z","iopub.execute_input":"2025-06-15T08:19:50.546572Z","iopub.status.idle":"2025-06-15T08:19:50.659169Z","shell.execute_reply.started":"2025-06-15T08:19:50.546549Z","shell.execute_reply":"2025-06-15T08:19:50.658402Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:19:52.720804Z","iopub.execute_input":"2025-06-15T08:19:52.721075Z","iopub.status.idle":"2025-06-15T08:19:52.730959Z","shell.execute_reply.started":"2025-06-15T08:19:52.721056Z","shell.execute_reply":"2025-06-15T08:19:52.730092Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                                 English  \\\n0      The Indian independence movement was a signifi...   \n1      Mahatma Gandhi emerged as the leader of the In...   \n2      The Salt March of 1930, led by Mahatma Gandhi,...   \n3      The Quit India Movement, launched in 1942, was...   \n4      Jawaharlal Nehru, the first Prime Minister of ...   \n...                                                  ...   \n15210  The Assam State Textbook Production and Public...   \n15211  The state's economic development is supported ...   \n15212  The government has launched various initiative...   \n15213  Assam's renewable energy sector is growing, wi...   \n15214  The state aims to increase the share of renewa...   \n\n                                                    Bodo  \n0      भारतनि उदांस्रि सोमावसारनाया भारतनि जारिमिनाव ...  \n1      महात्मा गान्धीआ भारतारि उदांस्रि सोमावसारनायनि...  \n2      महात्मा गान्धीनि दैदेन्नायाव 1930 मायथाइनि संख...  \n3      1942 मायथाइयाव जागायजेन्नाय भारत नागार सोमावसा...  \n4      उदां भारतनि गिबि गाहाय मन्थ्रि जवाहरलाल नेहरुआ...  \n...                                                  ...  \n15210  आसाम रायजो फराय बिजाब दिहुन्नाय आरो पाब्लिकेसन...  \n15211  रायजोनि रांखान्थियारि जौगानाया बिनि मिथिंगायार...  \n15212  सोरखारा आबहावा संरैखा खालामनाय आरो अरायथा जौगा...  \n15213  सानखरां, बार आरो दैमोब्लिब मावथांखिफोराव रां थ...  \n15214  बे रायजोनि थांखिया गासै गोहो गलायनायाव गोदान ग...  \n\n[15215 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>English</th>\n      <th>Bodo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The Indian independence movement was a signifi...</td>\n      <td>भारतनि उदांस्रि सोमावसारनाया भारतनि जारिमिनाव ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Mahatma Gandhi emerged as the leader of the In...</td>\n      <td>महात्मा गान्धीआ भारतारि उदांस्रि सोमावसारनायनि...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The Salt March of 1930, led by Mahatma Gandhi,...</td>\n      <td>महात्मा गान्धीनि दैदेन्नायाव 1930 मायथाइनि संख...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The Quit India Movement, launched in 1942, was...</td>\n      <td>1942 मायथाइयाव जागायजेन्नाय भारत नागार सोमावसा...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Jawaharlal Nehru, the first Prime Minister of ...</td>\n      <td>उदां भारतनि गिबि गाहाय मन्थ्रि जवाहरलाल नेहरुआ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15210</th>\n      <td>The Assam State Textbook Production and Public...</td>\n      <td>आसाम रायजो फराय बिजाब दिहुन्नाय आरो पाब्लिकेसन...</td>\n    </tr>\n    <tr>\n      <th>15211</th>\n      <td>The state's economic development is supported ...</td>\n      <td>रायजोनि रांखान्थियारि जौगानाया बिनि मिथिंगायार...</td>\n    </tr>\n    <tr>\n      <th>15212</th>\n      <td>The government has launched various initiative...</td>\n      <td>सोरखारा आबहावा संरैखा खालामनाय आरो अरायथा जौगा...</td>\n    </tr>\n    <tr>\n      <th>15213</th>\n      <td>Assam's renewable energy sector is growing, wi...</td>\n      <td>सानखरां, बार आरो दैमोब्लिब मावथांखिफोराव रां थ...</td>\n    </tr>\n    <tr>\n      <th>15214</th>\n      <td>The state aims to increase the share of renewa...</td>\n      <td>बे रायजोनि थांखिया गासै गोहो गलायनायाव गोदान ग...</td>\n    </tr>\n  </tbody>\n</table>\n<p>15215 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"\ndataset = load_dataset(\n    'csv', \n    data_files=\"bodo.csv\", \n    split=\"train\"\n)\n\nsubset = dataset.shuffle(seed=42).select(range(15215 ))\n\ndataset_split = subset.train_test_split(test_size=0.1) \ntrain_dataset = dataset_split['train']\ntest_dataset = dataset_split['test']\n\nprint(f\"Train examples: {len(train_dataset)}, Test examples: {len(test_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:19:55.551147Z","iopub.execute_input":"2025-06-15T08:19:55.551499Z","iopub.status.idle":"2025-06-15T08:19:55.922215Z","shell.execute_reply.started":"2025-06-15T08:19:55.551477Z","shell.execute_reply":"2025-06-15T08:19:55.921638Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53e56a8d24f24c0181bdbd5f8262a38a"}},"metadata":{}},{"name":"stdout","text":"Train examples: 13693, Test examples: 1522\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"def normalize_text(text):\n    if text is None:\n        return \"\"\n    \n    text = text.strip()\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(r'[^\\x20-\\x7E\\u0900-\\u097F\\u0980-\\u09FF\\u0A80-\\u0AFF\\u1C00-\\u1C4F]', '', text)\n    text = text.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\").replace(\"‘\", \"'\")\n    \n    return text\ndef filter_missing(example):\n    return example[\"English\"] is not None and example[\"Bodo\"] is not None\n\ntrain_dataset = train_dataset.filter(filter_missing)\ntest_dataset = test_dataset.filter(filter_missing)\ntrain_dataset = train_dataset.map(preprocess_function)\ntest_dataset = test_dataset.map(preprocess_function)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:19:58.221395Z","iopub.execute_input":"2025-06-15T08:19:58.221670Z","iopub.status.idle":"2025-06-15T08:19:58.444542Z","shell.execute_reply.started":"2025-06-15T08:19:58.221652Z","shell.execute_reply":"2025-06-15T08:19:58.443622Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/13693 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84d23a4cf3ee49d2b9ca2b37a3b2a3f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1522 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3037e9454ba24c02b4e2acb1827d977a"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3303113379.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'preprocess_function' is not defined"],"ename":"NameError","evalue":"name 'preprocess_function' is not defined","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:20:01.761341Z","iopub.execute_input":"2025-06-15T08:20:01.762100Z","iopub.status.idle":"2025-06-15T08:20:01.767562Z","shell.execute_reply.started":"2025-06-15T08:20:01.762071Z","shell.execute_reply":"2025-06-15T08:20:01.766817Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['English', 'Bodo'],\n    num_rows: 13692\n})"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"import pandas as pd\n\ndata = pd.DataFrame(train_dataset[:10])  # Load first 10 rows\ndata\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:20:05.595886Z","iopub.execute_input":"2025-06-15T08:20:05.596641Z","iopub.status.idle":"2025-06-15T08:20:05.605093Z","shell.execute_reply.started":"2025-06-15T08:20:05.596614Z","shell.execute_reply":"2025-06-15T08:20:05.604478Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                             English  \\\n0  Nature’s cycles of growth, decay, and renewal ...   \n1  China and the United States have the highest n...   \n2  E-commerce transformed retail by allowing busi...   \n3  He is of the opinion that filmmakers only targ...   \n4          Representations of binary tree in memory.   \n5  The celebration of Bwisagu is a time of joy wh...   \n6  The Bodo people have a deep connection to thei...   \n7  Delhi LG’s Proposal Return: Delhi LG Vinai Kum...   \n8  Cultural Exchange: Sharing of cultures through...   \n9  Modi’s government faces opposition challenges ...   \n\n                                                Bodo  \n0  मिथिंगानि जौगानाय, जोबस्रांनाय आरो गोदान खालाम...  \n1  चीन आरो जुथाइ रायजो आमेरिकायाव बुहुमाव बयनिख्र...  \n2  ई-कमर्सआ फालांगिफोरखौ मुलुगनाङै बायगिरिफोरसिम ...  \n3  बिथांनि साननाया बेनो दि सावथुन बानायगिरिफोरा ग...  \n4              मेम'रिआव बाइनेरि ट्रीनि दिन्थिफुंनाय।  \n5  बैसागुनि फालिथाया रंजानायनि मोनसे सम जेराव नखर...  \n6  बर' सुबुंफोरा गावसोरनि हाजों गोथौ सोमोन्दो दं ...  \n7  दिल्ली एलजीनि प्रप'जल रिटार्न: दिल्लीनि एल.जि....  \n8  हारिमुआरि सोलाय-सोल': सावथुन, देंखो आरो बिजोंन...  \n9  म'दिनि सोरखारा रांखान्थियारि बिथांखिफोरखौ लाना...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>English</th>\n      <th>Bodo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Nature’s cycles of growth, decay, and renewal ...</td>\n      <td>मिथिंगानि जौगानाय, जोबस्रांनाय आरो गोदान खालाम...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>China and the United States have the highest n...</td>\n      <td>चीन आरो जुथाइ रायजो आमेरिकायाव बुहुमाव बयनिख्र...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>E-commerce transformed retail by allowing busi...</td>\n      <td>ई-कमर्सआ फालांगिफोरखौ मुलुगनाङै बायगिरिफोरसिम ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>He is of the opinion that filmmakers only targ...</td>\n      <td>बिथांनि साननाया बेनो दि सावथुन बानायगिरिफोरा ग...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Representations of binary tree in memory.</td>\n      <td>मेम'रिआव बाइनेरि ट्रीनि दिन्थिफुंनाय।</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>The celebration of Bwisagu is a time of joy wh...</td>\n      <td>बैसागुनि फालिथाया रंजानायनि मोनसे सम जेराव नखर...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>The Bodo people have a deep connection to thei...</td>\n      <td>बर' सुबुंफोरा गावसोरनि हाजों गोथौ सोमोन्दो दं ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Delhi LG’s Proposal Return: Delhi LG Vinai Kum...</td>\n      <td>दिल्ली एलजीनि प्रप'जल रिटार्न: दिल्लीनि एल.जि....</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Cultural Exchange: Sharing of cultures through...</td>\n      <td>हारिमुआरि सोलाय-सोल': सावथुन, देंखो आरो बिजोंन...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Modi’s government faces opposition challenges ...</td>\n      <td>म'दिनि सोरखारा रांखान्थियारि बिथांखिफोरखौ लाना...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_name = \"ai4bharat/indictrans2-en-indic-1B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\ndef preprocess_examples(examples):\n    inputs = [f\"eng_Latn brx_Deva {en}\" for en in examples[\"English\"]]\n    targets = examples[\"Bodo\"]\n\n    model_inputs = tokenizer(\n        inputs,\n        max_length=512,\n        padding=\"longest\",\n        truncation=True\n    )\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets,\n            max_length=512,\n            padding=\"longest\",\n            truncation=True\n        )\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n  \n# Then proceed with dataset processing\ntrain_dataset = train_dataset.map(preprocess_examples, batched=True, remove_columns=train_dataset.column_names)\ntest_dataset = test_dataset.map(preprocess_examples, batched=True, remove_columns=test_dataset.column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:20:08.646019Z","iopub.execute_input":"2025-06-15T08:20:08.646386Z","iopub.status.idle":"2025-06-15T08:20:18.432728Z","shell.execute_reply.started":"2025-06-15T08:20:08.646361Z","shell.execute_reply":"2025-06-15T08:20:18.432147Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69caa026ec3e4af1a7bca1f21f813310"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenization_indictrans.py:   0%|          | 0.00/8.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f765919875374b62b842362d045f2960"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-en-indic-1B:\n- tokenization_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"dict.SRC.json:   0%|          | 0.00/645k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe619652477349f7a14898980b21a0c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dict.TGT.json:   0%|          | 0.00/3.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7009494b25294b67a7863b9cf39a108c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.SRC:   0%|          | 0.00/759k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2db7d894d42451caac8e3bfa8a31ecc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.TGT:   0%|          | 0.00/3.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2216b90570d74f44930dbfdac5315829"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c55385fa3474199b44c26c6a65e1217"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13692 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02bd1e11cdb44666b8b5c959ee547efe"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1522 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bbf861e076840e4abbce69897823859"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig\nimport torch\nimport torch.nn as nn\n\nbnb_config = BitsAndBytesConfig(load_in_8bit=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    \"ai4bharat/indictrans2-en-indic-1B\",\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    trust_remote_code=True,  \n    #device_map=\"auto\"        \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:20:18.433819Z","iopub.execute_input":"2025-06-15T08:20:18.434045Z","iopub.status.idle":"2025-06-15T08:20:40.165216Z","shell.execute_reply.started":"2025-06-15T08:20:18.434028Z","shell.execute_reply":"2025-06-15T08:20:40.164467Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84e60f68179e4adeb98c128be6287026"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_indictrans.py:   0%|          | 0.00/14.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7307f617f22a4cab8c4febe2daf81a8f"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-en-indic-1B:\n- configuration_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_indictrans.py:   0%|          | 0.00/79.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc474a4cbb964289b0096ec281fa86dd"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-en-indic-1B:\n- modeling_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61119090eb304cd5a4dbaebf8316278e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"153a9cbe1f9946cf98be680cda21bb3f"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n\n# Set pad token if missing\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Your input sentence\nenglish_text = \"The weather is very good today.\"\n\n# Format input as per IndicTrans2 requirements\nformatted_input = f\"eng_Latn brx_Deva {english_text}\"\n\n# Tokenize\ninputs = tokenizer(\n    formatted_input,\n    return_tensors=\"pt\",\n    padding=\"longest\",\n    truncation=True\n)\n\n# Generate translation\noutput = model.generate(\n    **inputs,\n    max_length=128,\n    num_beams=5,\n    early_stopping=True\n)\n\n# Decode output\nbodo_translation = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n\nprint(\"English:\", english_text)\nprint(\"Bodo:\", bodo_translation)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:20:40.166072Z","iopub.execute_input":"2025-06-15T08:20:40.166301Z","iopub.status.idle":"2025-06-15T08:20:40.265831Z","shell.execute_reply.started":"2025-06-15T08:20:40.166284Z","shell.execute_reply":"2025-06-15T08:20:40.264038Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3223394820.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Generate translation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m output = model.generate(\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"encoder_outputs\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2279\u001b[0m             \u001b[0;31m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2280\u001b[0;31m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[1;32m   2281\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2282\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/ai4bharat/indictrans2-en-indic-1B/10e65a9951a1e922cd109a95e8aba9357b62144b/modeling_indictrans.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"],"ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"from peft import LoraConfig, TaskType, get_peft_model\n\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,\n    r=64,\n    lora_alpha=128,\n    target_modules=[\"q_proj\", \"v_proj\",\"k_proj\",\"dense\"], \n    lora_dropout=0.1,\n    bias=\"none\",\n    use_dora=True\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:20:51.320927Z","iopub.execute_input":"2025-06-15T08:20:51.321694Z","iopub.status.idle":"2025-06-15T08:20:52.123905Z","shell.execute_reply.started":"2025-06-15T08:20:51.321671Z","shell.execute_reply":"2025-06-15T08:20:52.123267Z"}},"outputs":[{"name":"stdout","text":"trainable params: 21,399,552 || all params: 1,136,943,104 || trainable%: 1.8822\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"device = torch.device(\"cuda\")\n\nmodel=model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:20:54.271039Z","iopub.execute_input":"2025-06-15T08:20:54.271786Z","iopub.status.idle":"2025-06-15T08:20:54.302826Z","shell.execute_reply.started":"2025-06-15T08:20:54.271753Z","shell.execute_reply":"2025-06-15T08:20:54.302027Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"data_collator = DataCollatorForSeq2Seq(\n        tokenizer=tokenizer,\n        model=model,\n        padding=\"longest\",\n        pad_to_multiple_of=8, \n        label_pad_token_id=-100\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:20:56.361101Z","iopub.execute_input":"2025-06-15T08:20:56.361479Z","iopub.status.idle":"2025-06-15T08:20:56.366682Z","shell.execute_reply.started":"2025-06-15T08:20:56.361453Z","shell.execute_reply":"2025-06-15T08:20:56.365750Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:20:58.631106Z","iopub.execute_input":"2025-06-15T08:20:58.631938Z","iopub.status.idle":"2025-06-15T08:20:58.635661Z","shell.execute_reply.started":"2025-06-15T08:20:58.631914Z","shell.execute_reply":"2025-06-15T08:20:58.634918Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"bleu_metric = evaluate.load(\"sacrebleu\")\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\n\n# def transliterate_predictions(preds):\n#     # Transliterate from Devanagari to Bengali script\n#     return [transliterate(pred, sanscript.DEVANAGARI, sanscript.BENGALI) for pred in preds]\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds.predictions, eval_preds.label_ids\n    \n    if isinstance(preds, tuple):\n        preds = preds[0]\n    if preds.ndim == 3:\n        preds = np.argmax(preds, axis=-1)\n\n    labels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Transliterate predicted Devanagari output to Bengali script\n    # decoded_preds = transliterate_predictions(decoded_preds)\n\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n    return {\"bleu\": result[\"score\"]}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:21:01.816143Z","iopub.execute_input":"2025-06-15T08:21:01.816434Z","iopub.status.idle":"2025-06-15T08:21:02.422949Z","shell.execute_reply.started":"2025-06-15T08:21:01.816415Z","shell.execute_reply":"2025-06-15T08:21:02.422431Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f23f790751b341febd94b0f1cc108251"}},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./output\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=2,\n    num_train_epochs=1,\n    learning_rate=3e-5, \n    warmup_steps=200, # for entire dataset 1000\n    lr_scheduler_type=\"cosine\", \n    logging_steps=50, # for entire dataset 500\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n   # save_steps=1000,\n    save_total_limit=2,\n    fp16=True,\n    predict_with_generate=True,\n    report_to=\"none\",\n    optim=\"adamw_torch_fused\",\n    adam_beta1=0.9,\n    adam_beta2=0.98,\n    ddp_find_unused_parameters=False,\n    remove_unused_columns=False,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"bleu\",\n    greater_is_better=True,\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\nmodel.config.use_cache = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:21:05.116289Z","iopub.execute_input":"2025-06-15T08:21:05.116568Z","iopub.status.idle":"2025-06-15T08:21:05.180007Z","shell.execute_reply.started":"2025-06-15T08:21:05.116549Z","shell.execute_reply":"2025-06-15T08:21:05.179426Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/3633880997.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\nNo label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"trainer.train()   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:21:08.884750Z","iopub.execute_input":"2025-06-15T08:21:08.885369Z","iopub.status.idle":"2025-06-15T08:52:53.153560Z","shell.execute_reply.started":"2025-06-15T08:21:08.885344Z","shell.execute_reply":"2025-06-15T08:52:53.152804Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='856' max='856' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [856/856 31:41, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>7.583100</td>\n      <td>7.431449</td>\n      <td>17.591270</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=856, training_loss=8.230269053271998, metrics={'train_runtime': 1903.7445, 'train_samples_per_second': 7.192, 'train_steps_per_second': 0.45, 'total_flos': 6052875643256832.0, 'train_loss': 8.230269053271998, 'epoch': 1.0})"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"predict_results = trainer.predict(test_dataset)\npredictions = predict_results.predictions\nlabels = predict_results.label_ids\n\n# If predictions is tuple, take first element\nif isinstance(predictions, tuple):\n    predictions = predictions[0]\n\n# If predictions are logits (batch_size, seq_len, vocab_size), convert to token IDs\nif predictions.ndim == 3:\n    predictions = np.argmax(predictions, axis=-1)\n\n# Replace -100 in labels (ignored tokens) with pad_token_id for decoding\nlabels = np.where(labels == -100, tokenizer.pad_token_id, labels)\n\n# Decode to text\ndecoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\ndecoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n# Transliterate predicted Devanagari output to Bengali script for Assamese\n# decoded_preds = transliterate_predictions(decoded_preds)\n\n# Postprocess (strip whitespace etc)\ndecoded_preds, processed_references = postprocess_text(decoded_preds, decoded_labels)\n\n# Compute BLEU score\nresult = bleu_metric.compute(predictions=decoded_preds, references=processed_references)\nbleu_score = result[\"score\"]\n\nprint(f\"SacreBLEU Score: {bleu_score:.4f}\")\nprint(f\"Detailed BLEU: {result}\")\n\nprint(\"\\nSample Actual vs Predicted Translations:\")\nfor i in range(min(5, len(decoded_labels))):\n    print(f\"  Example {i+1}:\")\n    print(f\"    Actual:    {decoded_labels[i]}\")\n    print(f\"    Predicted: {decoded_preds[i]}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:52:56.981348Z","iopub.execute_input":"2025-06-15T08:52:56.981650Z","iopub.status.idle":"2025-06-15T08:59:30.251085Z","shell.execute_reply.started":"2025-06-15T08:52:56.981628Z","shell.execute_reply":"2025-06-15T08:59:30.250362Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/bitsandbytes/autograd/_functions.py:185: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"SacreBLEU Score: 17.5913\nDetailed BLEU: {'score': 17.59126996196025, 'counts': [8762, 4538, 2454, 1393], 'totals': [17794, 16272, 14752, 13233], 'precisions': [49.24131729796561, 27.888397246804328, 16.635032537960953, 10.526713519232223], 'bp': 0.7943826938525097, 'sys_len': 17794, 'ref_len': 21890}\n\nSample Actual vs Predicted Translations:\n  Example 1:\n    Actual:    पि. एम जानो लुबैनायाव जेबो गोरोन्थि गैयाः नायडु\n    Predicted: पि जानो गोसोयाव जेबो गोरोन्थि गैयाः नायडु\n\n  Example 2:\n    Actual:    इउ.के. टेक्स अथ'रिटिआ भेट होफिन्नाय गोबावनि गेजेराव उफ्रा फुंखानि थाखाय रादाय होयो\n    Predicted: \" वेटखौ सुख'नायाव इउके टेक्स अथ'रिटिआ बांद्राय फुंखा होनो गनायथि होयो\n\n  Example 3:\n    Actual:    मोब्लिबारि कारफोरा जोरनो हानाय आरो खहागोनां हगारनायखौ बाङाय खालामनानै अट'म'टिभ दारिमिनखौ सोलायहोदोंमोन।\n    Predicted: मोब्लिबजों सालायजानाय कारफोरा जोर होयो आरो खहा खालामग्रा बिदुं ओंखारनायखौ खम खालामो ।\n\n  Example 4:\n    Actual:    आमेरिकानि गाहाय नोगोरमाफोराव कभिड-19 टिका थुनायनि कभारेजआ 90 जौखोन्दोसिम सौहैबाय\n    Predicted: आमेरिकानि गाहाय नोगोरफोराव क'विड - 19 टिका होनायनि गोसारथिया 90% सिम सौहैबाय\n\n  Example 5:\n    Actual:    शंकरदेव सोमावसारनाया अस'मिया हारिमुआरि हुदाफोर आरो इसोरारि जिउआव गोहोम खोख्लैदोंमोन।\n    Predicted: शंकरदेव जांख्रिखांनाया असमिया हारिमुआरि हुदाफोर आरो धोरोमारि जिउखौ गोहोम खोख्लैदोंमोन ।\n\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Save final model and tokenizer\ntrainer.save_model(\"./final_model\")\ntokenizer.save_pretrained(\"./final_model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:59:30.252412Z","iopub.execute_input":"2025-06-15T08:59:30.252674Z","iopub.status.idle":"2025-06-15T08:59:30.921692Z","shell.execute_reply.started":"2025-06-15T08:59:30.252657Z","shell.execute_reply":"2025-06-15T08:59:30.920945Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"('./final_model/tokenizer_config.json',\n './final_model/special_tokens_map.json',\n './final_model/dict.SRC.json',\n './final_model/dict.TGT.json',\n './final_model/model.SRC',\n './final_model/model.TGT',\n './final_model/added_tokens.json')"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive(\"/kaggle/working/final_model\", 'zip', \"./final_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T08:59:30.922537Z","iopub.execute_input":"2025-06-15T08:59:30.922792Z","iopub.status.idle":"2025-06-15T08:59:35.944354Z","shell.execute_reply.started":"2025-06-15T08:59:30.922774Z","shell.execute_reply":"2025-06-15T08:59:35.943435Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/final_model.zip'"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"test_file_path = \"/kaggle/input/emnlp-test-enbodo-2025/en-bodo.txt\"\n\n# Read English lines\nenglish_sentences = []\nwith open(test_file_path, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        parts = line.strip().split(\"\\t\")\n        if len(parts) >= 1:\n            english_sentences.append(parts[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:31:49.359151Z","iopub.execute_input":"2025-06-15T09:31:49.359472Z","iopub.status.idle":"2025-06-15T09:31:49.365746Z","shell.execute_reply.started":"2025-06-15T09:31:49.359450Z","shell.execute_reply":"2025-06-15T09:31:49.365022Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\npretrained_tokenizer_name = \"ai4bharat/indictrans2-en-indic-1B\"\nmodel_path = \"./final_model\"\n\n# Load tokenizer from original repo (with trust_remote_code)\ntokenizer = AutoTokenizer.from_pretrained(pretrained_tokenizer_name, trust_remote_code=True)\n\n# Load fine-tuned model from local checkpoint\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n\nmodel.eval().cuda()  # if GPU available\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:31:53.463075Z","iopub.execute_input":"2025-06-15T09:31:53.463798Z","iopub.status.idle":"2025-06-15T09:32:03.512071Z","shell.execute_reply.started":"2025-06-15T09:31:53.463775Z","shell.execute_reply":"2025-06-15T09:32:03.511514Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"The repository for ai4bharat/indictrans2-en-indic-1B contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/ai4bharat/indictrans2-en-indic-1B.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"IndicTransForConditionalGeneration(\n  (model): IndicTransModel(\n    (encoder): IndicTransEncoder(\n      (embed_tokens): Embedding(32322, 1024, padding_idx=1)\n      (embed_positions): IndicTransSinusoidalPositionalEmbedding()\n      (layers): ModuleList(\n        (0-17): 18 x IndicTransEncoderLayer(\n          (self_attn): IndicTransAttention(\n            (k_proj): lora.Linear(\n              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.1, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=1024, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=1024, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n              (lora_magnitude_vector): ModuleDict(\n                (default): lora.dora.DoraLinearLayer()\n              )\n            )\n            (v_proj): lora.Linear(\n              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.1, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=1024, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=1024, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n              (lora_magnitude_vector): ModuleDict(\n                (default): lora.dora.DoraLinearLayer()\n              )\n            )\n            (q_proj): lora.Linear(\n              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.1, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=1024, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=1024, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n              (lora_magnitude_vector): ModuleDict(\n                (default): lora.dora.DoraLinearLayer()\n              )\n            )\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=8192, bias=True)\n          (fc2): Linear(in_features=8192, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): IndicTransDecoder(\n      (embed_tokens): Embedding(122672, 1024, padding_idx=1)\n      (embed_positions): IndicTransSinusoidalPositionalEmbedding()\n      (layers): ModuleList(\n        (0-17): 18 x IndicTransDecoderLayer(\n          (self_attn): IndicTransAttention(\n            (k_proj): lora.Linear(\n              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.1, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=1024, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=1024, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n              (lora_magnitude_vector): ModuleDict(\n                (default): lora.dora.DoraLinearLayer()\n              )\n            )\n            (v_proj): lora.Linear(\n              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.1, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=1024, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=1024, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n              (lora_magnitude_vector): ModuleDict(\n                (default): lora.dora.DoraLinearLayer()\n              )\n            )\n            (q_proj): lora.Linear(\n              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.1, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=1024, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=1024, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n              (lora_magnitude_vector): ModuleDict(\n                (default): lora.dora.DoraLinearLayer()\n              )\n            )\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): IndicTransAttention(\n            (k_proj): lora.Linear(\n              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.1, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=1024, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=1024, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n              (lora_magnitude_vector): ModuleDict(\n                (default): lora.dora.DoraLinearLayer()\n              )\n            )\n            (v_proj): lora.Linear(\n              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.1, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=1024, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=1024, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n              (lora_magnitude_vector): ModuleDict(\n                (default): lora.dora.DoraLinearLayer()\n              )\n            )\n            (q_proj): lora.Linear(\n              (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.1, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=1024, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=1024, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n              (lora_magnitude_vector): ModuleDict(\n                (default): lora.dora.DoraLinearLayer()\n              )\n            )\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1024, out_features=8192, bias=True)\n          (fc2): Linear(in_features=8192, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=1024, out_features=122672, bias=False)\n)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"import torch\n\ndef batch_translate(sentences, model, tokenizer, batch_size=16, src_lang=\"eng_Latn\", tgt_lang=\"brx_Deva\"):\n    all_outputs = []\n    for i in range(0, len(sentences), batch_size):\n        batch = sentences[i:i+batch_size]\n        inputs = [f\"{src_lang} {tgt_lang} {text}\" for text in batch]\n        encoded = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n\n        with torch.no_grad():\n            outputs = model.generate(\n                **encoded,\n                max_length=128,\n                num_beams=5,\n                early_stopping=True\n            )\n        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        all_outputs.extend(decoded)\n    return all_outputs\n\n# Generate predictions\nassamese_predictions = batch_translate(english_sentences, model, tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:32:11.184318Z","iopub.execute_input":"2025-06-15T09:32:11.184567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save to myresult.txt\nwith open(\"myresult.txt\", \"w\", encoding=\"utf-8\") as f:\n    for translation in assamese_predictions:\n        f.write(translation.strip() + \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-15T09:30:59.127175Z","iopub.status.idle":"2025-06-15T09:30:59.127478Z","shell.execute_reply.started":"2025-06-15T09:30:59.127366Z","shell.execute_reply":"2025-06-15T09:30:59.127380Z"}},"outputs":[],"execution_count":null}]}